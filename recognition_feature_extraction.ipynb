{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3695a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "from Datasets.Morph2.DataParser import DataParser\n",
    "#from Datasets.Morph2.Morph2RecognitionDataset import Morph2RecognitionDataset\n",
    "from Datasets.Morph2.Morph2RecognitionIdxDataset import Morph2RecognitionIdxDataset \n",
    "from Models.ArcMarginClassifier import ArcMarginClassifier\n",
    "from Optimizers.RangerLars import RangerLars\n",
    "from Training.train_recognition_model import train_recognition_model\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_parser = DataParser('./Datasets/Morph2/aligned_data/aligned_dataset_with_metadata_uint8.hdf5')\n",
    "data_parser.initialize_data()\n",
    "\n",
    "ids_train = np.unique([json.loads(m)['id_num'] for m in data_parser.y_train])\n",
    "ids_test = np.unique([json.loads(m)['id_num'] for m in data_parser.y_test])\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data_parser.x_train, data_parser.y_train, test_size=0.33, random_state=42)\n",
    "x_train, y_train, x_test, y_test = data_parser.x_train,\tdata_parser.y_train, data_parser.x_test, data_parser.y_test,\n",
    "\n",
    "train_ds = Morph2RecognitionIdxDataset(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    ids_train,\n",
    "    transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(224, (0.9, 1.0)),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ColorJitter(\n",
    "#             brightness=0.125,\n",
    "#             contrast=0.125,\n",
    "#             saturation=0.125,\n",
    "#             hue=0.125\n",
    "#         ),\n",
    "#         transforms.RandomAffine(\n",
    "#             degrees=15,\n",
    "#             translate=(0.15, 0.15),\n",
    "#             scale=(0.85, 1.15),\n",
    "#             shear=15,\n",
    "#             resample=Image.BICUBIC\n",
    "#         ),\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        # transforms.RandomErasing(p=0.5, scale=(0.02, 0.25))\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_ds = Morph2RecognitionIdxDataset(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    ids_test,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_ds,\n",
    "    'val': test_ds\n",
    "}\n",
    "\n",
    "data_loaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "# Create model and parameters\n",
    "model = ArcMarginClassifier(len(ids_train))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "pretrained_model_path = 'F:/age_estimation_with_error_estimator/weights/Morph2_recognition/vgg16/RangerLars_unfreeze_at_15_lr_1e2_steplr_01_batchsize_64'\n",
    "pretrained_model_file = os.path.join(pretrained_model_path, \"weights.pt\")\n",
    "model.load_state_dict(torch.load(pretrained_model_file), strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71030e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_net.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c607d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchextractor as tx\n",
    "\n",
    "model_ext = tx.Extractor(model, [\"base_net.classifier.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec969de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ext.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444c7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e6866f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1384/1384 [03:09<00:00,  7.30it/s]\n"
     ]
    }
   ],
   "source": [
    "face2emb_arr_trn = []\n",
    "for i, batch in enumerate(tqdm(data_loaders['train'])):\n",
    "    faces = batch['image'].to(device)\n",
    "    with torch.no_grad():\n",
    "        output, features = model_ext(faces)\n",
    "    \n",
    "    for j in range(len(output)):\n",
    "        face2emb_arr_trn.append(features['base_net.classifier.5'][j].cpu().numpy().reshape(1, 4096))\n",
    "        \n",
    "#     if i == 5:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5bcc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('face2emb_arr_trn_recog.npy', np.array(face2emb_arr_trn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32dcad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 332/332 [00:44<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "face2emb_arr_vld = []\n",
    "for i, batch in enumerate(tqdm(data_loaders['val'])):\n",
    "    faces = batch['image'].to(device)\n",
    "    with torch.no_grad():\n",
    "        output, features = model_ext(faces)\n",
    "    \n",
    "    for j in range(len(output)):\n",
    "        face2emb_arr_vld.append(features['base_net.classifier.5'][j].cpu().numpy().reshape(1, 4096))\n",
    "        \n",
    "#     if i == 5:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61859c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('face2emb_arr_vld_recog.npy', np.array(face2emb_arr_vld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283ca5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "face2emb_arr_trn_r = np.load('face2emb_arr_trn_recog.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd08841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn equal\n"
     ]
    }
   ],
   "source": [
    "eq = True\n",
    "for i in range(len(face2emb_arr_trn_r)):\n",
    "    if not np.array_equal(face2emb_arr_trn_r[i], face2emb_arr_trn[i]):\n",
    "        eq = False\n",
    "        break\n",
    "        \n",
    "if len(face2emb_arr_trn_r) != len(face2emb_arr_trn):\n",
    "    eq = False\n",
    "    \n",
    "if eq:\n",
    "    print(\"trn equal\")\n",
    "else:\n",
    "    print(\"trn not eq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f241e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "face2emb_arr_vld_r = np.load('face2emb_arr_vld_recog.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1dd707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vld equal\n"
     ]
    }
   ],
   "source": [
    "eq = True\n",
    "for i in range(len(face2emb_arr_vld_r)):\n",
    "    if not np.array_equal(face2emb_arr_vld_r[i], face2emb_arr_vld[i]):\n",
    "        print(\"trn not eq\")\n",
    "        eq = False\n",
    "        break\n",
    "\n",
    "        \n",
    "if len(face2emb_arr_vld_r) != len(face2emb_arr_vld):\n",
    "    eq = False\n",
    "    \n",
    "if eq:\n",
    "    print(\"vld equal\")\n",
    "else:\n",
    "    print(\"vld not eq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434313a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
